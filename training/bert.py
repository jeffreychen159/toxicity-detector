# -*- coding: utf-8 -*-
"""bert.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1klSER8GJnOPJCe3AGzT1WmTGeUpP_h53

# BERT Toxicity Classification with Comprehensive Evaluation

This notebook trains a BERT model on the Jigsaw toxicity dataset and provides comprehensive evaluation metrics including cross-domain testing on Reddit data.
"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
from torch import nn
from torch.amp import autocast, GradScaler
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd
import time
import json

TOKENIZER_NAME = "google-bert/bert-base-cased"
MODEL_NAME = "google-bert/bert-base-cased"

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)
print("Tokenizer loaded.")

def encode_texts(texts, max_len=64):
    return tokenizer(
        list(texts),
        padding="max_length",
        truncation=True,
        max_length=max_len,
        return_tensors="pt"
    )

class BertDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": self.labels[idx]
        }

def train_bert(train_texts, train_labels, dev_texts, dev_labels,
               effective_batch_size=64,
               epochs=3,
               gpu_batch_size=16,
               label_cols=['toxic','severe_toxic','obscene','threat','insult','identity_hate']
               ):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)
    torch.manual_seed(42)

    accumulation_steps = max(1, effective_batch_size // gpu_batch_size)

    print("Encoding text...")
    train_enc = encode_texts(train_texts)
    dev_enc = encode_texts(dev_texts)

    print("Creating datasets...")
    train_dataset = BertDataset(train_enc, train_labels)
    dev_dataset = BertDataset(dev_enc, dev_labels)

    train_loader = DataLoader(train_dataset, batch_size=gpu_batch_size,
                              shuffle=True, num_workers=2, pin_memory=True)
    dev_loader = DataLoader(dev_dataset, batch_size=gpu_batch_size,
                            shuffle=False, num_workers=2, pin_memory=True)

    print("Initializing BERT model...")
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=train_labels.shape[1],
        problem_type="multi_label_classification"
        )
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    scaler = GradScaler()

    print("Starting training...\n")

    for epoch in range(1, epochs + 1):
        start_time = time.time()
        model.train()
        total_loss = 0

        optimizer.zero_grad()

        ### Begin training ###
        for i, batch in enumerate(train_loader):
            input_ids = batch["input_ids"].to(device, non_blocking=True)
            attention_mask = batch["attention_mask"].to(device, non_blocking=True)
            labels = batch["labels"].to(device, non_blocking=True)

            with autocast(device_type="cuda"):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss / accumulation_steps

            scaler.scale(loss).backward()
            total_loss += loss.item() * accumulation_steps

            if (i + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

        avg_train_loss = total_loss / len(train_loader)

        ### Validation ###
        model.eval()
        dev_loss = 0
        preds_list, labels_list, probs_list = [], [], []

        with torch.no_grad():
            for batch in dev_loader:
                input_ids = batch["input_ids"].to(device, non_blocking=True)
                attention_mask = batch["attention_mask"].to(device, non_blocking=True)
                labels = batch["labels"].to(device, non_blocking=True)

                with autocast(device_type="cuda"):
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                    logits = outputs.logits

                dev_loss += loss.item()

                probs = torch.sigmoid(logits).cpu().numpy()
                preds = (probs > 0.5).astype(int)

                probs_list.append(probs)
                preds_list.append(preds)
                labels_list.append(labels.cpu().numpy())

        preds = np.vstack(preds_list)
        labels = np.vstack(labels_list)

        # FIX: Average the dev_loss
        avg_dev_loss = dev_loss / len(dev_loader)

        ### Metrics ###
        f1_micro = f1_score(labels, preds, average="micro")
        dev_acc = (preds == labels).mean()
        exact_match = np.mean(np.all(preds == labels, axis=1))

        print(f"\nEpoch {epoch}/{epochs} | "
              f"train_loss={avg_train_loss:.4f} | "
              f"dev_loss={avg_dev_loss:.4f} | "
              f"dev_f1_micro={f1_micro:.4f} | "
              f"dev_acc={dev_acc:.4f} | "
              f"exact_match={exact_match:.4f} | "
              f"time={time.time() - start_time:.1f} sec")

        print("Per-label metrics:")
        for j, label_name in enumerate(label_cols):
            y_true = labels[:, j]
            y_pred = preds[:, j]

            f1 = f1_score(y_true, y_pred, zero_division=0)
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            support = y_true.sum()

            print(f"{label_name}: "
                  f"F1={f1:.4f}, "
                  f"Precision={precision:.4f}, "
                  f"Recall={recall:.4f}, "
                  f"Support={support}")

    # Return model and data for comprehensive evaluation
    return model, dev_loader, device, label_cols

from google.colab import drive
drive.mount('/content/drive')

def prepare_labels(df):
    label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']
    for c in label_cols:
        if c not in df.columns:
            raise ValueError(f"Expected label column '{c}' in the CSV")
    y = df[label_cols].values.astype(np.float32)
    return y, label_cols

def prepare_X(df):
    texts = df['comment_text'].fillna("").values
    return texts

def prepare_bert_dropped():
    # Load CSV
    df = pd.read_csv("/content/drive/MyDrive/datasets/train.csv")

    # Prepare labels
    y, label_cols = prepare_labels(df)
    texts = prepare_X(df)

    nontoxic_mask = (y.sum(axis=1) == 0)
    toxic_mask = ~nontoxic_mask

    nontoxic_indices = np.where(nontoxic_mask)[0]
    toxic_indices = np.where(toxic_mask)[0]

    np.random.seed(42)
    num_to_drop = int(0.25 * len(nontoxic_indices))

    drop_indices = np.random.choice(
        nontoxic_indices,
        size=num_to_drop,
        replace=False
    )

    keep_mask = np.ones(len(df), dtype=bool)
    keep_mask[drop_indices] = False

    texts = texts[keep_mask]
    y = y[keep_mask]

    print(f"Original dataset size: {len(df)}")
    print(f"Non-toxic samples: {len(nontoxic_indices)}")
    print(f"Dropped non-toxic samples: {num_to_drop}")
    print(f"Final dataset size: {len(texts)}")

    train_texts, val_texts, y_train, y_val = train_test_split(
        texts, y, test_size=0.2, random_state=42, shuffle=True
    )

    # Train and return model + data for evaluation
    model, dev_loader, device, label_cols = train_bert(train_texts, y_train, val_texts, y_val)

    return model, dev_loader, device, label_cols

def compute_comprehensive_metrics(model, dev_loader, device, label_cols, threshold=0.5):
    """
    Compute comprehensive metrics including ROC-AUC and confusion matrices.
    """
    model.eval()
    all_probs = []
    all_preds = []
    all_labels = []
    total_loss = 0.0

    print("\nComputing comprehensive metrics...")
    with torch.no_grad():
        for batch in dev_loader:
            input_ids = batch["input_ids"].to(device, non_blocking=True)
            attention_mask = batch["attention_mask"].to(device, non_blocking=True)
            labels = batch["labels"].to(device, non_blocking=True)

            with autocast(device_type="cuda"):
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                logits = outputs.logits

            total_loss += loss.item()

            probs = torch.sigmoid(logits).cpu().numpy()
            preds = (probs >= threshold).astype(np.int32)

            all_probs.append(probs)
            all_preds.append(preds)
            all_labels.append(labels.cpu().numpy())

    probs = np.vstack(all_probs)
    preds = np.vstack(all_preds)
    labels = np.vstack(all_labels).astype(np.int32)
    avg_loss = total_loss / len(dev_loader)

    # Overall metrics
    micro_f1 = f1_score(labels, preds, average="micro", zero_division=0)
    macro_f1 = f1_score(labels, preds, average="macro", zero_division=0)
    macro_precision = precision_score(labels, preds, average="macro", zero_division=0)
    macro_recall = recall_score(labels, preds, average="macro", zero_division=0)
    element_accuracy = (preds == labels).mean()
    exact_match = (preds == labels).all(axis=1).mean()

    try:
        roc_auc_macro = roc_auc_score(labels, probs, average="macro")
    except ValueError:
        roc_auc_macro = float("nan")

    # Per-class metrics
    per_class_f1 = f1_score(labels, preds, average=None, zero_division=0)
    per_class_precision = precision_score(labels, preds, average=None, zero_division=0)
    per_class_recall = recall_score(labels, preds, average=None, zero_division=0)

    try:
        per_class_roc = roc_auc_score(labels, probs, average=None)
    except ValueError:
        per_class_roc = [float("nan")] * len(label_cols)

    per_class_support = labels.sum(axis=0)

    # Confusion matrices
    confusion_matrices = {}
    for idx, label_name in enumerate(label_cols):
        cm = confusion_matrix(labels[:, idx], preds[:, idx], labels=[0, 1]).ravel()
        if len(cm) == 4:
            tn, fp, fn, tp = cm
        else:
            tn = fp = fn = tp = 0
        confusion_matrices[label_name] = {
            "tn": int(tn), "fp": int(fp), "fn": int(fn), "tp": int(tp)
        }

    # Compile metrics
    metrics = {
        "loss": float(avg_loss),
        "micro_f1": float(micro_f1),
        "macro_f1": float(macro_f1),
        "macro_precision": float(macro_precision),
        "macro_recall": float(macro_recall),
        "element_accuracy": float(element_accuracy),
        "exact_match": float(exact_match),
        "roc_auc_macro": float(roc_auc_macro),
        "threshold": threshold,
        "per_class_metrics": {}
    }

    for idx, label_name in enumerate(label_cols):
        metrics["per_class_metrics"][label_name] = {
            "f1": float(per_class_f1[idx]),
            "precision": float(per_class_precision[idx]),
            "recall": float(per_class_recall[idx]),
            "roc_auc": float(per_class_roc[idx]),
            "support": int(per_class_support[idx]),
            "confusion_matrix": confusion_matrices[label_name]
        }

    return metrics


def print_metrics(metrics):
    """Pretty print comprehensive metrics."""
    print("\n" + "="*80)
    print("COMPREHENSIVE EVALUATION METRICS")
    print("="*80)

    print(f"\nOverall Metrics (threshold={metrics['threshold']}):")
    print(f"  Loss:              {metrics['loss']:.4f}")
    print(f"  Micro F1:          {metrics['micro_f1']:.4f}")
    print(f"  Macro F1:          {metrics['macro_f1']:.4f}")
    print(f"  Macro Precision:   {metrics['macro_precision']:.4f}")
    print(f"  Macro Recall:      {metrics['macro_recall']:.4f}")
    print(f"  Element Accuracy:  {metrics['element_accuracy']:.4f}")
    print(f"  Exact Match:       {metrics['exact_match']:.4f}")
    print(f"  ROC-AUC (Macro):   {metrics['roc_auc_macro']:.4f}")

    print("\n" + "-"*80)
    print("Per-Class Metrics:")
    print("-"*80)

    for label_name, class_metrics in metrics["per_class_metrics"].items():
        print(f"\n{label_name.upper()}:")
        print(f"  F1:          {class_metrics['f1']:.4f}")
        print(f"  Precision:   {class_metrics['precision']:.4f}")
        print(f"  Recall:      {class_metrics['recall']:.4f}")
        print(f"  ROC-AUC:     {class_metrics['roc_auc']:.4f}")
        print(f"  Support:     {class_metrics['support']}")
        cm = class_metrics['confusion_matrix']
        print(f"  Confusion Matrix: TN={cm['tn']}, FP={cm['fp']}, FN={cm['fn']}, TP={cm['tp']}")

    print("\n" + "="*80)

# Run training and capture outputs
print("Starting training with comprehensive evaluation...\n")
model, dev_loader, device, label_cols = prepare_bert_dropped()

print("\n" + "="*80)
print("TRAINING COMPLETE - Computing comprehensive metrics...")
print("="*80)

# Compute comprehensive metrics
metrics = compute_comprehensive_metrics(
    model=model,
    dev_loader=dev_loader,
    device=device,
    label_cols=label_cols,
    threshold=0.5
)

# Print formatted metrics
print_metrics(metrics)

# Save to JSON
output_path = "/content/drive/MyDrive/eval_metrics.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(metrics, f, indent=2)

print(f"\nMetrics saved to: {output_path}")

# Print:
print("\n" + "-"*80)
print("Model Storage Information:")
print("-"*80)
print("BERT-base-cased model size: ~420-440 MB")
print("This includes:")
print("  - Model weights: ~420 MB")
print("  - Tokenizer files: ~1-2 MB")
print("  - Config files: <1 MB")
print("\nTo save the model, uncomment the lines below:")
print("-"*80)

# Save the model:
model_save_path = "/content/drive/MyDrive/bert_toxicity_model"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"\nModel saved to: {model_save_path}")

"""# Cross-Domain Evaluation: Reddit Data

This section evaluates the Wikipedia-trained BERT model on Reddit toxicity data.
"""

from datasets import load_dataset

print("="*80)
print("CROSS-DOMAIN EVALUATION: REDDIT DATA")
print("="*80)

# Check if required variables exist
if 'model' not in globals() or 'device' not in globals() or 'metrics' not in globals():
    print("\nERROR: Please run the training cells first!")
    print("The following variables are required: model, device, metrics")
    raise RuntimeError("Training must be completed before cross-domain evaluation")

# Load Real Toxicity Prompts dataset
print("\nLoading Real Toxicity Prompts dataset from HuggingFace...")
dataset = load_dataset("allenai/real-toxicity-prompts", split="train")
print(f"Loaded {len(dataset):,} samples")

# Convert to pandas
df_reddit = pd.DataFrame(dataset)

# Extract toxicity scores
print("\nProcessing toxicity scores...")

def extract_toxicity(example):
    """Extract toxicity score from prompt or continuation"""
    if example.get('prompt') and isinstance(example['prompt'], dict):
        toxicity = example['prompt'].get('toxicity')
        text = example['prompt'].get('text', '')
    elif example.get('continuation') and isinstance(example['continuation'], dict):
        toxicity = example['continuation'].get('toxicity')
        text = example['continuation'].get('text', '')
    else:
        toxicity = None
        text = ''
    return {'text': text, 'toxicity_score': toxicity}

# Process all examples
processed = [extract_toxicity(ex) for ex in dataset]
reddit_df = pd.DataFrame(processed)

# Filter out examples without toxicity scores or empty text
reddit_df = reddit_df[reddit_df['toxicity_score'].notna()]
reddit_df = reddit_df[reddit_df['text'].str.len() > 0]

print(f"Processed {len(reddit_df):,} samples with toxicity scores")

# Convert toxicity scores to binary labels
reddit_df['binary_label'] = (reddit_df['toxicity_score'] >= 0.5).astype(int)

print(f"\nDataset Statistics:")
print(f"  Total samples: {len(reddit_df):,}")
print(f"  Toxic samples: {reddit_df['binary_label'].sum():,} ({reddit_df['binary_label'].mean()*100:.1f}%)")
print(f"  Non-toxic samples: {(reddit_df['binary_label'] == 0).sum():,} ({(reddit_df['binary_label'] == 0).mean()*100:.1f}%)")
print(f"  Mean toxicity score: {reddit_df['toxicity_score'].mean():.3f}")

# Sample for evaluation
EVAL_SAMPLES = min(10000, len(reddit_df))
reddit_sample = reddit_df.sample(n=EVAL_SAMPLES, random_state=42).reset_index(drop=True)
print(f"\nSampled {EVAL_SAMPLES:,} examples for evaluation")

# Tokenize Reddit data
print("\nðŸ”¤ Tokenizing Reddit text...")
reddit_texts = reddit_sample['text'].tolist()
reddit_labels = reddit_sample['binary_label'].values

reddit_encodings = tokenizer(
    reddit_texts,
    truncation=True,
    max_length=64,
    padding='max_length',
    return_tensors='pt'
)

# Create dataset
class RedditDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": self.labels[idx]
        }

reddit_dataset = RedditDataset(reddit_encodings, reddit_labels)
reddit_loader = DataLoader(
    reddit_dataset,
    batch_size=64,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

print(f"Created Reddit dataset with {len(reddit_dataset)} samples")

# Run predictions
print("\nRunning predictions on Reddit data...")

model.eval()
all_probs_reddit = []
all_labels_reddit = []

with torch.no_grad():
    for batch in reddit_loader:
        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        labels = batch["labels"].to(device, non_blocking=True)

        with autocast(device_type="cuda"):
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs_reddit.append(probs)
        all_labels_reddit.append(labels.cpu().numpy())

all_probs_reddit = np.vstack(all_probs_reddit)
all_labels_reddit = np.concatenate(all_labels_reddit)

# Use first label (toxic) for binary classification
toxic_probs = all_probs_reddit[:, 0]
preds_reddit = (toxic_probs >= 0.5).astype(int)

print("Predictions complete")

# Compute metrics
acc_reddit = accuracy_score(all_labels_reddit, preds_reddit)
prec_reddit = precision_score(all_labels_reddit, preds_reddit, zero_division=0)
rec_reddit = recall_score(all_labels_reddit, preds_reddit, zero_division=0)
f1_reddit = f1_score(all_labels_reddit, preds_reddit, zero_division=0)
auc_reddit = roc_auc_score(all_labels_reddit, toxic_probs)

cm_reddit = confusion_matrix(all_labels_reddit, preds_reddit)
tn_reddit, fp_reddit, fn_reddit, tp_reddit = cm_reddit.ravel()

reddit_metrics = {
    'dataset': 'Real Toxicity Prompts (Reddit)',
    'model': 'BERT-base-cased (trained on Wikipedia/Jigsaw)',
    'samples_evaluated': EVAL_SAMPLES,
    'accuracy': float(acc_reddit),
    'precision': float(prec_reddit),
    'recall': float(rec_reddit),
    'f1': float(f1_reddit),
    'roc_auc': float(auc_reddit),
    'confusion_matrix': {
        'tn': int(tn_reddit),
        'fp': int(fp_reddit),
        'fn': int(fn_reddit),
        'tp': int(tp_reddit)
    },
    'support': {
        'non_toxic': int((all_labels_reddit == 0).sum()),
        'toxic': int((all_labels_reddit == 1).sum())
    }
}

print("\n" + "="*80)
print("CROSS-DOMAIN RESULTS: REDDIT TOXICITY DETECTION")
print("="*80)
print(json.dumps(reddit_metrics, indent=2))

print("\n" + "="*80)
print("CONFUSION MATRIX (REDDIT)")
print("="*80)
print(f"True Negatives:  {tn_reddit:,}")
print(f"False Positives: {fp_reddit:,}")
print(f"False Negatives: {fn_reddit:,}")
print(f"True Positives:  {tp_reddit:,}")

# Domain Gap Analysis
print("\n" + "="*80)
print("DOMAIN GAP ANALYSIS: WIKIPEDIA (JIGSAW) vs REDDIT")
print("="*80)

jigsaw_toxic_metrics = metrics['per_class_metrics']['toxic']
jigsaw_acc = metrics['element_accuracy']
jigsaw_f1 = jigsaw_toxic_metrics['f1']
jigsaw_prec = jigsaw_toxic_metrics['precision']
jigsaw_rec = jigsaw_toxic_metrics['recall']
jigsaw_auc = jigsaw_toxic_metrics['roc_auc']

print(f"{'Metric':<20} {'Jigsaw (Wikipedia)':<20} {'Reddit':<20} {'Gap':<15}")
print("-"*75)
print(f"{'Accuracy':<20} {jigsaw_acc:<20.4f} {acc_reddit:<20.4f} {jigsaw_acc-acc_reddit:+.4f}")
print(f"{'Precision':<20} {jigsaw_prec:<20.4f} {prec_reddit:<20.4f} {jigsaw_prec-prec_reddit:+.4f}")
print(f"{'Recall':<20} {jigsaw_rec:<20.4f} {rec_reddit:<20.4f} {jigsaw_rec-rec_reddit:+.4f}")
print(f"{'F1 Score':<20} {jigsaw_f1:<20.4f} {f1_reddit:<20.4f} {jigsaw_f1-f1_reddit:+.4f}")
print(f"{'ROC-AUC':<20} {jigsaw_auc:<20.4f} {auc_reddit:<20.4f} {jigsaw_auc-auc_reddit:+.4f}")

# Performance drop analysis
f1_drop = jigsaw_f1 - f1_reddit
drop_pct = (f1_drop / jigsaw_f1) * 100

print(f"\nðŸ“‰ Performance Drop:")
print(f"  F1 Score decreased by {f1_drop:.4f} ({drop_pct:.1f}%)")

if drop_pct < 5:
    print(f"Excellent generalization! Model performs similarly on Reddit data.")
elif drop_pct < 15:
    print(f"Good generalization with minor domain gap.")
else:
    print(f"Significant domain gap detected. Model struggles with Reddit-style language.")

# Save results
reddit_save_path = "/content/drive/MyDrive/reddit_cross_domain_results.json"
with open(reddit_save_path, 'w') as f:
    json.dump(reddit_metrics, f, indent=2)
print(f"\nReddit results saved to: {reddit_save_path}")

# Save comparison
comparison_results = {
    'jigsaw_wikipedia': {
        'accuracy': float(jigsaw_acc),
        'precision': float(jigsaw_prec),
        'recall': float(jigsaw_rec),
        'f1': float(jigsaw_f1),
        'roc_auc': float(jigsaw_auc)
    },
    'reddit': reddit_metrics,
    'domain_gap': {
        'f1_drop': float(f1_drop),
        'f1_drop_percentage': float(drop_pct),
        'accuracy_gap': float(jigsaw_acc - acc_reddit),
        'precision_gap': float(jigsaw_prec - prec_reddit),
        'recall_gap': float(jigsaw_rec - rec_reddit),
        'roc_auc_gap': float(jigsaw_auc - auc_reddit)
    }
}

comparison_save_path = "/content/drive/MyDrive/domain_comparison_results.json"
with open(comparison_save_path, 'w') as f:
    json.dump(comparison_results, f, indent=2)
print(f"âœ… Domain comparison saved to: {comparison_save_path}")

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import json
import numpy as np
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
import pandas as pd

print("Loading trained model and metrics...")

# Load the tokenizer (needed for all evaluations)
TOKENIZER_NAME = "google-bert/bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Try to load saved model first
model_save_path = "/content/drive/MyDrive/bert_toxicity_model"
import os

if os.path.exists(model_save_path) and os.path.exists(f"{model_save_path}/config.json"):
    print(f"âœ… Loading saved model from {model_save_path}")
    model = AutoModelForSequenceClassification.from_pretrained(model_save_path)
    model.to(device)
    model.eval()
    print("âœ… Model loaded successfully!")
else:
    print(f"Saved model not found at {model_save_path}")
    raise FileNotFoundError("Model not found. Please train or provide correct model path.")

# Load metrics
metrics_path = "/content/drive/MyDrive/eval_metrics.json"
if os.path.exists(metrics_path):
    with open(metrics_path, "r") as f:
        metrics = json.load(f)
    print(f"Metrics loaded from {metrics_path}")
else:
    print(f"Metrics not found at {metrics_path} - will skip domain gap analysis")
    metrics = None

# Load helper functions
def prepare_labels(df):
    label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']
    for c in label_cols:
        if c not in df.columns:
            raise ValueError(f"Expected label column '{c}' in the CSV")
    y = df[label_cols].values.astype(np.float32)
    return y, label_cols

def prepare_X(df):
    texts = df['comment_text'].fillna("").values
    return texts

print(f"\nSetup complete!")
print(f"   Device: {device}")
print(f"   Model parameters: {sum(p.numel() for p in model.parameters()):,}")

# Analysis with the binary toxicity classification with the wikipedia dataset
print("="*80)
print("BINARY TOXICITY CLASSIFICATION EVALUATION")
print("="*80)

# Check if required variables exist
if 'model' not in globals() or 'device' not in globals():
    print("\nERROR: Please run the training cells first!")
    print("The following variables are required: model, device")
    raise RuntimeError("Training must be completed before binary classification evaluation")

print("\nConverting multi-label predictions to binary toxicity classification...")

# We'll use the validation data from training
# Need to recreate the validation dataset
df = pd.read_csv("/content/drive/MyDrive/datasets/train.csv")

LABEL_COLS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

# Prepare data (same as training)
y, _ = prepare_labels(df)
texts = prepare_X(df)

# Apply same dropping as training
nontoxic_mask = (y.sum(axis=1) == 0)
nontoxic_indices = np.where(nontoxic_mask)[0]
np.random.seed(42)
num_to_drop = int(0.25 * len(nontoxic_indices))
drop_indices = np.random.choice(nontoxic_indices, size=num_to_drop, replace=False)
keep_mask = np.ones(len(df), dtype=bool)
keep_mask[drop_indices] = False
texts = texts[keep_mask]
y = y[keep_mask]

# Same train/val split as training
_, val_texts, _, val_labels = train_test_split(
    texts, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"Validation set size: {len(val_texts):,}")

# Tokenize validation texts
val_encodings = tokenizer(
    list(val_texts),
    truncation=True,
    max_length=64,
    padding='max_length',
    return_tensors='pt'
)

# Create dataset
class BinaryEvalDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.encodings["input_ids"][idx],
            "attention_mask": self.encodings["attention_mask"][idx],
            "labels": self.labels[idx]
        }

val_dataset = BinaryEvalDataset(val_encodings, val_labels)
val_loader = DataLoader(
    val_dataset,
    batch_size=64,
    shuffle=False,
    num_workers=2,
    pin_memory=True
)

# Run predictions
print("\nRunning predictions for binary classification...")

model.eval()
all_probs = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        labels = batch["labels"].to(device, non_blocking=True)

        with autocast(device_type="cuda"):
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

        probs = torch.sigmoid(logits).cpu().numpy()
        all_probs.append(probs)
        all_labels.append(labels.cpu().numpy())

all_probs = np.vstack(all_probs)
all_labels = np.vstack(all_labels).astype(np.int32)

# Convert multi-label to binary (any toxic label = toxic)
binary_true = (all_labels.sum(axis=1) > 0).astype(int)
binary_pred = (all_probs.max(axis=1) >= 0.5).astype(int)
binary_probs = all_probs.max(axis=1)

# Compute binary metrics
acc = accuracy_score(binary_true, binary_pred)
prec = precision_score(binary_true, binary_pred, zero_division=0)
rec = recall_score(binary_true, binary_pred, zero_division=0)
f1 = f1_score(binary_true, binary_pred, zero_division=0)
auc = roc_auc_score(binary_true, binary_probs)

cm = confusion_matrix(binary_true, binary_pred)
tn, fp, fn, tp = cm.ravel()

binary_metrics = {
    'model': 'BERT-base-cased (trained on Wikipedia/Jigsaw)',
    'validation_samples': len(val_texts),
    'accuracy': float(acc),
    'precision': float(prec),
    'recall': float(rec),
    'f1': float(f1),
    'roc_auc': float(auc),
    'confusion_matrix': {
        'tn': int(tn),
        'fp': int(fp),
        'fn': int(fn),
        'tp': int(tp)
    },
    'support': {
        'non_toxic': int((binary_true == 0).sum()),
        'toxic': int((binary_true == 1).sum())
    }
}

print("\n" + "="*80)
print("BINARY TOXICITY CLASSIFICATION RESULTS")
print("="*80)
print(json.dumps(binary_metrics, indent=2))

print("\n" + "="*80)
print("CONFUSION MATRIX (BINARY)")
print("="*80)
print(f"True Negatives:  {tn:,}")
print(f"False Positives: {fp:,}")
print(f"False Negatives: {fn:,}")
print(f"True Positives:  {tp:,}")

# Save results
binary_save_path = "/content/drive/MyDrive/binary_classification_results.json"
with open(binary_save_path, 'w') as f:
    json.dump(binary_metrics, f, indent=2)
print(f"\nBinary classification results saved to: {binary_save_path}")