Epoch 1/4 | train_loss=0.0675 | dev_loss=0.0613 | dev_f1=0.6525 | dev_acc=0.9793 | exact_match=0.9155 | time=5.48 seconds
  toxic           | F1: 0.6895 | Acc: 0.9517
  severe_toxic    | F1: 0.1676 | Acc: 0.9907
  obscene         | F1: 0.7533 | Acc: 0.9767
  threat          | F1: 0.2022 | Acc: 0.9978
  insult          | F1: 0.6177 | Acc: 0.9680
  identity_hate   | F1: 0.0588 | Acc: 0.9910
Epoch 2/4 | train_loss=0.0579 | dev_loss=0.0616 | dev_f1=0.6919 | dev_acc=0.9793 | exact_match=0.9105 | time=4.82 seconds
  toxic           | F1: 0.7204 | Acc: 0.9506
  severe_toxic    | F1: 0.3132 | Acc: 0.9904
  obscene         | F1: 0.7693 | Acc: 0.9762
  threat          | F1: 0.3360 | Acc: 0.9974
  insult          | F1: 0.6768 | Acc: 0.9700
  identity_hate   | F1: 0.3047 | Acc: 0.9911
Epoch 3/4 | train_loss=0.0545 | dev_loss=0.0607 | dev_f1=0.6804 | dev_acc=0.9794 | exact_match=0.9124 | time=5.13 seconds
  toxic           | F1: 0.7172 | Acc: 0.9512
  severe_toxic    | F1: 0.1285 | Acc: 0.9902
  obscene         | F1: 0.7581 | Acc: 0.9769
  threat          | F1: 0.3009 | Acc: 0.9975
  insult          | F1: 0.6655 | Acc: 0.9692
  identity_hate   | F1: 0.2575 | Acc: 0.9915
Epoch 4/4 | train_loss=0.0506 | dev_loss=0.0599 | dev_f1=0.6826 | dev_acc=0.9798 | exact_match=0.9163 | time=5.00 seconds
  toxic           | F1: 0.7138 | Acc: 0.9534
  severe_toxic    | F1: 0.2250 | Acc: 0.9903
  obscene         | F1: 0.7570 | Acc: 0.9768
  threat          | F1: 0.3902 | Acc: 0.9977
  insult          | F1: 0.6739 | Acc: 0.9688
  identity_hate   | F1: 0.3081 | Acc: 0.9917

============================================================
SUBTASK A: BINARY TOXICITY CLASSIFICATION (MLP)
============================================================
{
  "model": "mlp_model.pth",
  "sample_fraction": 1,
  "validation_samples": 15958,
  "accuracy": 0.9115177340518862,
  "precision": 0.578790141896938,
  "recall": 0.47751078250154033,
  "f1": 0.5232950708980418,
  "roc_auc": 0.8418742307615436,
  "confusion_matrix": {
    "tn": 13771,
    "fp": 564,
    "fn": 848,
    "tp": 775
  },
  "support": {
    "non_toxic": 14335,
    "toxic": 1623
  }
}

============================================================
CONFUSION MATRIX
============================================================
True Negatives:  13,771
False Positives: 564
False Negatives: 848
True Positives:  775


============================================================
CROSS-DOMAIN RESULTS: MLP ON REDDIT TOXICITY DETECTION
============================================================
{
  "dataset": "Real Toxicity Prompts (Reddit)",
  "model": "mlp_model.pth",
  "samples_evaluated": 10000,
  "accuracy": 0.7422,
  "precision": 0.2756756756756757,
  "recall": 0.11789181692094314,
  "f1": 0.16515544041450778,
  "roc_auc": 0.4978650769955646,
  "confusion_matrix": {
    "tn": 7167,
    "fp": 670,
    "fn": 1908,
    "tp": 255
  },
  "support": {
    "non_toxic": 7837,
    "toxic": 2163
  }
}

============================================================
CONFUSION MATRIX
============================================================
True Negatives:  7,167
False Positives: 670
False Negatives: 1,908
True Positives:  255